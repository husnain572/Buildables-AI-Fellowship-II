{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ec81b5",
   "metadata": {},
   "source": [
    "# Understanding Transformer Architecture\n",
    "\n",
    "## What is a Transformer?\n",
    "\n",
    "The Transformer is a neural network architecture introduced in the paper \"Attention is All You Need\" (2017). It revolutionized natural language processing by using self-attention mechanisms instead of recurrent layers.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. Self-Attention Mechanism\n",
    "- **Purpose**: Allows the model to focus on different parts of the input sequence\n",
    "- **How it works**: Each token can attend to all other tokens in the sequence\n",
    "- **Formula**: `Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V`\n",
    "\n",
    "### 2. Multi-Head Attention\n",
    "- Uses multiple attention heads in parallel\n",
    "- Each head learns different types of relationships\n",
    "- Outputs are concatenated and projected\n",
    "\n",
    "### 3. Feed-Forward Networks\n",
    "- Simple neural networks applied to each position\n",
    "- Two linear transformations with ReLU activation\n",
    "- Same across all positions but different parameters\n",
    "\n",
    "### 4. Positional Encoding\n",
    "- Since Transformers don't have inherent sequence order\n",
    "- Adds information about token positions\n",
    "- Uses sine and cosine functions of different frequencies\n",
    "\n",
    "## Architecture Flow\n",
    "\n",
    "1. **Input Embedding**: Convert tokens to dense vectors\n",
    "2. **Positional Encoding**: Add position information\n",
    "3. **Encoder Layers**: Stack of attention + feed-forward\n",
    "4. **Decoder Layers**: Similar to encoder but with masked attention\n",
    "5. **Output Layer**: Linear projection + softmax\n",
    "\n",
    "## Why Transformers Work\n",
    "\n",
    "- **Parallelization**: Unlike RNNs, can process all positions simultaneously\n",
    "- **Long-range Dependencies**: Self-attention connects distant tokens\n",
    "- **Scalability**: Performance improves with more data and parameters\n",
    "\n",
    "## Modern Variations\n",
    "\n",
    "### GPT (Generative Pre-trained Transformer)\n",
    "- Decoder-only architecture\n",
    "- Autoregressive generation\n",
    "- Pre-trained on next-token prediction\n",
    "\n",
    "### BERT (Bidirectional Encoder Representations)\n",
    "- Encoder-only architecture\n",
    "- Bidirectional context\n",
    "- Pre-trained on masked language modeling\n",
    "\n",
    "### T5 (Text-to-Text Transfer Transformer)\n",
    "- Encoder-decoder architecture\n",
    "- All tasks as text-to-text\n",
    "- Unified framework\n",
    "\n",
    "## Key Insights for Fellows\n",
    "\n",
    "1. **Attention is Key**: Understanding attention helps in prompt engineering\n",
    "2. **Context Window**: Limited by computational constraints\n",
    "3. **Training Objective**: Influences model capabilities\n",
    "4. **Scale Matters**: Larger models often perform better\n",
    "\n",
    "## Further Reading\n",
    "- [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "- [GPT-1 Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8fca6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
